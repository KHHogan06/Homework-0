---
title: "Breast Cancer Prediction Project"
author: "K.H. Hogan"
date: "1 November 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\  

  
## Introduction

This is an assignment from the HarvardX Machine Learning course. The goal is to train models using algorithms discussed during the course to make predications on a given dataset.

The brca dataset from the dslabs package contains information about breast cancer diagnosis biopsy samples for tumors that were determined to be either benign (not cancer) and malignant (cancer). The brca object is a list consisting of:

brca\$y: a vector of sample classifications ("B" = benign or "M" = malignant)  
brca\$x: a matrix of numeric features describing properties of the shape and size of cell nuclei extracted from biopsy microscope images

For these exercises, load the data by setting your options and loading the libraries and data as shown in the code here:

```{r libraries,  message = FALSE, error = FALSE,  warning = FALSE}

options(digits = 3)
library(matrixStats)
library(tidyverse)
library(caret)
library(dslabs)
data(brca)
```
\  

## Data Analysis  
  
  
### Dimensions and Properties
Prior to using an algorithm the data needs to be explored and basic knowledge obtained such as the number of observations and features, the proportion of positive outcomes and which features are most relevant.

How many samples are in the dataset? 

```{r samples}

dim(brca$x)[1]
```

How many predictors are in the matrix?
```{r predictors}

dim(brca$x)[2]
```
  
What proportion of the samples are malignant?

```{r proportion M}

mean(brca$y == "M")
```
  
  
Which column has the highest mean?  
```{r mean}

which.max(colMeans(brca$x))
which.min(colSds(brca$x))
```

Which column number has the lowest standard deviation? 
```{r sd}

which.min(colSds(brca$x))
```

\  

### Scaling the Matrix  
Scaling is an important concept in machine learning. For instance, clustering algorithms need features to be on the same scale to calculate distances between observations.  
  
  
Use sweep() two times to scale each column: subtract the column means of brca\$x, then divide by the column standard deviations of brca\$x.

```{r scaling}

x_mean_0 <- sweep(brca$x, 2, colMeans(brca$x))
x_scaled <- sweep(x_mean_0, 2, colSds(brca$x), FUN = "/")
```

After scaling, what is the standard deviation of the first column?

```{r scaled_sd}

sd(x_scaled[,1])
```
  
What is the median value of the first column?  
```{r scaled_median}

median(x_scaled[,1]) 
```  
\  

  
### Distance  

Calculate the distance between all samples using the scaled matrix.  

```{r calculate_dist}

d_samples <- dist(x_scaled)
``` 

What is the average distance between the first sample, which is benign, and other benign samples? 
 
```{r avg_dist_B}

dist_BtoB <- as.matrix(d_samples)[1, brca$y == "B"]
mean(dist_BtoB[2:length(dist_BtoB)]) 
```  

What is the average distance between the first sample and malignant samples?

```{r avg_dist_M}

dist_BtoM <- as.matrix(d_samples)[1, brca$y == "M"]
mean(dist_BtoM)
```
\  

  
### Heatmap of Features  

Heatmaps are visual representations of the row and column measures using color to show predictor relationships.  
  
  
Make a heatmap of the relationship between features using the scaled matrix

```{r heatmap}

d_features <- dist(t(x_scaled))
heatmap(as.matrix(d_features), labRow = NA, labCol = NA) 
```
\  

  
### Hierarchical Clustering  

Hierarchical clustering is a type of cluster analysis.  It begins with each observation as its own "group," it then iteratively joins the two closest groups together until all observations are then combined into one group.  The algorithm needs to be told either the number of groups to define or a minimum distance between observations to be grouped.  
  
Perform hierarchical clustering on the 30 features. Cut the tree into 5 groups.  

```{r hclust}

h <- hclust(d_features)
groups <- cutree(h, k = 5)
split(names(groups), groups) 
```
\  

  
### Principal Component Analysis  

In data sets with many predictors visualizing relationships is difficult and fitting models to all predictors leads to over-training.  For high-dimensional data using a smaller set of the most predictive features produces better models. The prcomp function performs principle components analysis which orders them by highest to lowest variance.  

Perform a principal component analysis of the scaled matrix. 

```{r pca}

pca <- prcomp(x_scaled)
summary(pca) 
```

What proportion of variance is explained by the first principal component? _0.443_   

How many principal components are required to explain at least 90% of the variance? _At PC7 cumulative variance is 91%_  
  
  
Plot the first two principal components with color representing tumor type (benign/malignant).

```{r pca_plot, fig.width=5, fig.height=3}

data.frame(pca$x[,1:2], type = brca$y) %>%
  ggplot(aes(PC1, PC2, color = type)) +
  geom_point()
```

Make a boxplot of the first 10 PCs grouped by tumor type.

```{r pca_boxplot, fig.width=5, fig.height=3}

data.frame(type = brca$y, pca$x[,1:10]) %>%
  gather(key = "PC", value = "value", -type) %>%
  ggplot(aes(PC, value, fill = type)) +
  geom_boxplot()
```


Which PCs are significantly different enough by tumor type that there is no overlap in the IQRs for benign and malignant samples? PC1


\  
\  
  
  
##  Training Models   

In order to train a model and test predictions the data must be split into two sets.
Set the seed to 1, then create a data partition splitting brca\$y and the scaled version of the brca$x matrix into 80/20 train/test set. 

```{r parition data, warning = FALSE}

set.seed(1, sample.kind = "Rounding")   
test_index <- createDataPartition(brca$y, times = 1, p = 0.2, list = FALSE)
test_x <- x_scaled[test_index,]
test_y <- brca$y[test_index]
train_x <- x_scaled[-test_index,]
train_y <- brca$y[-test_index]
```
  

Check that the training and test sets have similar proportions of benign and malignant tumors.

What proportion of the training set is benign?  `r mean(train_y == "B")`
What proportion of the test set is benign?  `r mean(test_y == "B")`  
\  

  
### K-Means Clustering  

K-means clustering defines $k$ clusters where each observations is assigned to the cluster whose center is the closest distance to the observation. 
  
The predict_kmeans() function defined here takes two arguments: a matrix of observations $x$ and a k-means object $k$ and assigns each row of $x$ to a cluster from $k$. Set the seed to 3. Perform k-means clustering on the training set with 2 centers and assign the output to k.
  
``` {r kmeans, warning = FALSE} 

predict_kmeans <- function(x, k) {
  centers <- k$centers    
  distances <- sapply(1:nrow(x), function(i){
    apply(centers, 1, function(y) dist(rbind(x[i,], y)))
  }) 
  max.col(-t(distances)) 
}

set.seed(3, sample.kind = "Rounding")   
k <- kmeans(train_x, centers = 2)
kmeans_preds <- ifelse(predict_kmeans(test_x, k) == 1, "B", "M")  
```  


What is the overall accuracy?

``` {r kmeans_acc}

mean(kmeans_preds == test_y)
```
  
  
What proportion of malignant tumors are correctly identified?  

``` {r kmeans_sens}

sensitivity(factor(kmeans_preds), test_y, positive = "B")
```
\  

  
### Logistic Regression

Logistic regression is regression model for a categorical outcome where outcome has been assigned values of 0 and 1 the regression is performed as if the outcome was continuous.  By applying a logistic transformation the prediction to log odds which is constrained between 0 and 1.  Logistic regression models are not very flexible 

Fit a logistic regression model on the training set with caret::train() using all predictors. 

``` {r glm, message = FALSE, error = FALSE,  warning = FALSE}

# caret package loaded
train_glm <- train(train_x, train_y, method = "glm")
```
  
  
What is the accuracy of the logistic regression model?  
``` {r glm_acc}

glm_preds <- predict(train_glm, test_x)
mean(glm_preds == test_y)
```
\  

  
###  Linear Discriminant Analysis (LDA) and  Quadratic Discriminant Analysis (QDA) 

QDA is a version a naive bayes where the distributions of the probability of outcomes (Y=1 and Y=0) are assumed to be multivariate normal. It estimates averages, standard deviations and correlations for each case for both outcomes. It does not work well with mmultiple predictors, and the distributions of outcomes is not always known. LDA, instead, assumes the correlation structure is the same for all classes which simplifies trying to correlate multiple predictors, but reduces the conditional probability of the outcome to a linear boundary which is not flexible.  
  
  
Train an LDA model and a QDA model on the training set.  Make predictions on the test set using each model.

``` {r qda_lda, warning=FALSE}

train_lda <- train(train_x, train_y, method = "lda")
train_qda <- train(train_x, train_y, method = "qda")
```

What is the accuracy of the LDA model?
``` {r qda_acc}

qda_preds <- predict(train_qda, test_x)
mean(qda_preds == test_y)
```

What is the accuracy of the LDA model?
``` {r lda_acc}

lda_preds <- predict(train_lda, test_x)
mean(lda_preds == test_y)
```
\  

  
### Loess Model  

LOESS is a type of local weighted regression (also called smoothing) than looks for trends in noisy data by approximating small "neighborhoods" of data points and defining a best-fit line between points.  The end results is a smoothed curve rather than a estimated straight line average.  LOESS assigns a reduced weight to data points further from the center of the neighborhood.  
  
  
Set the seed to 5, then fit a loess model using the caret package. You will need to install the gam package. 
Use the default tuning grid. This may take several minutes; ignore warnings.

``` {r loess, message = FALSE, error = FALSE,  warning = FALSE}

set.seed(5, sample.kind = "Rounding") 
train_loess <- train(train_x, train_y, method = "gamLoess")
```
  

What is the accuracy of the loess model?

``` {r loess_acc, message = FALSE, warning=FALSE}

loess_preds <- predict(train_loess, test_x)
mean(loess_preds == test_y)  
```
\  

  
  
### K-Nearest Neighbors Model  

K-Nearest Neighbors is a classification model that assumes similar observations are close to each other when distance is calculated.   kNN averages distances and assigns $k$ "neighborhoods" or clusters of similar observations.  Different distances calculations can be used such as Euclidean distance.  
  
  Set the seed to 7. Train a k-nearest neighbors model using the caret package. Try odd values of $k$  from 3 to 21. Use the final model to generate predictions. 
  
``` {r knn, warning = FALSE}  
set.seed(7, sample.kind = "Rounding") 
tuning <- data.frame(k = seq(3, 21, 2))
train_knn <- train(train_x, train_y,
                   method = "knn", 
                   tuneGrid = tuning)
```


What is the final value of k used in the model?  

``` {r knn_bestk}

train_knn$bestTune
```
  
  
What is the accuracy of the kNN model?

``` {r knn3}

knn_preds <- predict(train_knn, test_x)
mean(knn_preds == test_y)  
```
\  

  
### Random Forest Model  

Random Forest is a type of classification and regression tree (CART) algorithm which generates a large number of trees with smaller subsets of predictors then averages the outcomes.  

Set the seed to 9. Train a random forest model using the caret package. Test mtry values of c(3, 5, 7, 9). Use the argument importance = TRUE so that feature importance can be extracted.

``` {r randomforest, warning = FALSE}

set.seed(9, sample.kind = "Rounding") 
tuning <- data.frame(mtry = c(3, 5, 7, 9))
train_rf <- train(train_x, train_y,
                  method = "rf",
                  tuneGrid = tuning,
                  importance = TRUE)
```


What value of mtry gives the highest accuracy?  

``` {r rf_mtry}

train_rf$bestTune
```


What is the accuracy of the random forest model?  

``` {r rf_acc}

rf_preds <- predict(train_rf, test_x)
mean(rf_preds == test_y)
```
  
  
What is the most important variable?  

``` {r rf_avarImp}

varImp(train_rf)
```
_area_worst most important variable_

\  


### Creating an Ensemble  

Ensembles combine algorithms' results to achieve better predictions.  


Create an ensemble using the predictions from the 7 models created in the previous exercises: k-means, logistic regression, LDA, QDA, loess, k-nearest neighbors, and random forest. Use the ensemble to generate a majority prediction of the tumor type.

``` {r ensemble} 

ensemble <- cbind(glm = glm_preds=="B", lda = lda_preds=="B", qda = qda_preds=="B", loess = loess_preds=="B", 
                  rf = rf_preds=="B", knn = knn_preds=="B", kmeans = kmeans_preds=="B")

ensemble_preds <- ifelse(rowMeans(ensemble) > 0.5, "B", "M")
```


What is the accuracy of the ensemble prediction?

``` {r ensemble_acc} 

mean(ensemble_preds == test_y)
```
  
  
Make a table of the accuracies of the 7 models and the accuracy of the ensemble model.  

``` {r models_acc}

models <- c("K Means", "Logistic Regression", "LDA", "QDA", "Loess", "K-Nearest Neighbors", "Random Forest", "Ensemble")
accuracy <- c(mean(kmeans_preds == test_y),
              mean(glm_preds == test_y),
              mean(lda_preds == test_y),
              mean(qda_preds == test_y),
              mean(loess_preds == test_y),
              mean(knn_preds == test_y),
              mean(rf_preds == test_y),
              mean(ensemble_preds == test_y))
data.frame(Model = models, Accuracy = accuracy)
```

Which of these models has the highest accuracy? _LDA_
\  
\  

